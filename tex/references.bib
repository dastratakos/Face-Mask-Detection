@inproceedings{Fu_2017_CVPR,
    author = {Fu, Jianlong and Zheng, Heliang and Mei, Tao},
    title = {Look Closer to See Better: Recurrent Attention Convolutional Neural Network for Fine-Grained Image Recognition},
    booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {July},
    year = {2017},
    abstract = {Recognizing fine-grained categories (e.g., bird species) is difficult due to the challenges of discriminative region localization and fine-grained feature learning. Existing approaches predominantly solve these challenges independently, while neglecting the fact that region detection and fine-grained feature learning are mutually correlated and thus can reinforce each other. In this paper, we propose a novel recurrent attention convolutional neural network (RA-CNN) which recursively learns discriminative region attention and region-based feature representation at multiple scales in a mutual reinforced way. The learning at each scale consists of a classification sub-network and an attention proposal sub-network (APN). The APN starts from full images, and iteratively generates region attention from coarse to fine by taking previous prediction as a reference, while the finer scale network takes as input an amplified attended region from previous scale in a recurrent way. The proposed RA-CNN is optimized by an intra-scale classification loss and an inter-scale ranking loss, to mutually learn accurate region attention and fine-grained representation. RA-CNN does not need bounding box/part annotations and can be trained end-to-end. We conduct comprehensive experiments and show that RA-CNN achieves the best performance in three fine-grained tasks, with relative accuracy gains of 3.3%, 3.7%, 3.8%, on CUB Birds, Stanford Dogs and Stanford Cars, respectively.}
}

@inproceedings{10.1145/2647868.2654970,
    author = {Kagaya, Hokuto and Aizawa, Kiyoharu and Ogawa, Makoto},
    title = {Food Detection and Recognition Using Convolutional Neural Network},
    year = {2014},
    isbn = {9781450330633},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2647868.2654970},
    doi = {10.1145/2647868.2654970},
    abstract = {In this paper, we apply a convolutional neural network (CNN) to the tasks of detecting and recognizing food images. Because of the wide diversity of types of food, image recognition of food items is generally very difficult. However, deep learning has been shown recently to be a very powerful image recognition technique, and CNN is a state-of-the-art approach to deep learning. We applied CNN to the tasks of food detection and recognition through parameter optimization. We constructed a dataset of the most frequent food items in a publicly available food-logging system, and used it to evaluate recognition performance. CNN showed significantly higher accuracy than did traditional support-vector-machine-based methods with handcrafted features. In addition, we found that the convolution kernels show that color dominates the feature extraction process. For food image detection, CNN also showed significantly higher accuracy than a conventional method did.},
    booktitle = {Proceedings of the 22nd ACM International Conference on Multimedia},
    pages = {1085–1088},
    numpages = {4},
    keywords = {deep learning, food recognition, convolutional neural network, food detection},
    location = {Orlando, Florida, USA},
    series = {MM '14}
}

@article{554195,
    author={S. {Lawrence} and C. L. {Giles} and  {Ah Chung Tsoi} and A. D. {Back}},
    journal={IEEE Transactions on Neural Networks},
    title={Face recognition: a convolutional neural-network approach},
    year={1997},
    volume={8},
    number={1},
    pages={98-113},
    doi={10.1109/72.554195}
}

@inproceedings{Hijazi2015UsingCN,
  title={Using Convolutional Neural Networks for Image Recognition By},
  author={S. Hijazi and R. Kumar and C. Rowen},
  year={2015}
}

@article{10.1145/3065386,
    author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
    title = {ImageNet Classification with Deep Convolutional Neural Networks},
    year = {2017},
    issue_date = {June 2017},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {60},
    number = {6},
    issn = {0001-0782},
    url = {https://doi.org/10.1145/3065386},
    doi = {10.1145/3065386},
    abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.},
    journal = {Commun. ACM},
    month = may,
    pages = {84–90},
    numpages = {7}
}

@article{LOEY2021108288,
    title = "A hybrid deep transfer learning model with machine learning methods for face mask detection in the era of the COVID-19 pandemic",
    journal = "Measurement",
    volume = "167",
    pages = "108288",
    year = "2021",
    issn = "0263-2241",
    doi = "https://doi.org/10.1016/j.measurement.2020.108288",
    url = "http://www.sciencedirect.com/science/article/pii/S0263224120308289",
    author = "Mohamed Loey and Gunasekaran Manogaran and Mohamed Hamed N. Taha and Nour Eldeen M. Khalifa",
    keywords = "COVID-19, Masked face, Deep transfer learning, Classical machine learning",
    abstract = "The coronavirus COVID-19 pandemic is causing a global health crisis. One of the effective protection methods is wearing a face mask in public areas according to the World Health Organization (WHO). In this paper, a hybrid model using deep and classical machine learning for face mask detection will be presented. The proposed model consists of two components. The first component is designed for feature extraction using Resnet50. While the second component is designed for the classification process of face masks using decision trees, Support Vector Machine (SVM), and ensemble algorithm. Three face masked datasets have been selected for investigation. The Three datasets are the Real-World Masked Face Dataset (RMFD), the Simulated Masked Face Dataset (SMFD), and the Labeled Faces in the Wild (LFW). The SVM classifier achieved 99.64% testing accuracy in RMFD. In SMFD, it achieved 99.49%, while in LFW, it achieved 100% testing accuracy."
}

@misc{he2015deep,
    title={Deep Residual Learning for Image Recognition}, 
    author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
    year={2015},
    eprint={1512.03385},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@dataset{dataset,
    author = {Larxel},
    year = {2020},
    month = {05},
    day = {22},
    title = {Face Mask Detection},
    url = {https://www.kaggle.com/andrewmvd/face-mask-detection/metadata}
}